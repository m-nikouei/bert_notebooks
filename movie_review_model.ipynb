{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Movie Review Polarity Analysis Using TPU\n",
    "#### This is done base on https://colab.research.google.com/github/bentoml/gallery/blob/master/tensorflow/bert/bert_movie_reviews.ipynb#scrollTo=j0a4mTk9o1Qg\n",
    "\n",
    "This is a simplified version of the above notebook, where tokenizer and pad functions use built-in capabilities of pandas with arguably less complicated and unnecessary code that run faster.\n",
    "The model is the same as the notebook above. Two major changes are length of vectors which is changed to 512 and number of epochs which is increased to 10. The model is trained on TPUs. The model achieves 91.5% test accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU initialization\n",
    "\n",
    "We start by getting a TPU distributed strategy from the provided servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.3.0\n",
      "Python:  3.6.9 (default, Jul 17 2020, 12:50:27) \n",
      "[GCC 8.4.0]\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.67.152.218:8470\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.67.152.218:8470\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "## Initialize TPUs and setup env\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "print(\"Tensorflow: \", tf.__version__)\n",
    "print(\"Python: \", sys.version)\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: 100%|██████████| 12500/12500 [00:01<00:00, 7962.30it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:01<00:00, 9683.67it/s]\n",
      "pos: 100%|██████████| 12500/12500 [00:01<00:00, 8891.18it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:01<00:00, 8574.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "\n",
    "train = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                           \"aclImdb\", \"train\"))\n",
    "test = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                          \"aclImdb\", \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Counts\n",
    "\n",
    "Counting the number of elements in each class for train and test datasets, show a very balanced dataset. \n",
    "Here we are only modeling polarity, and it is a perfect split of negative and positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentiment: Counter({'1': 5100, '10': 4732, '8': 3009, '4': 2696, '7': 2496, '3': 2420, '2': 2284, '9': 2263})\n",
      "train polarity: Counter({1: 12500, 0: 12500})\n",
      "test polarity: Counter({'1': 5100, '10': 4732, '8': 3009, '4': 2696, '7': 2496, '3': 2420, '2': 2284, '9': 2263})\n",
      "test polarity: Counter({1: 12500, 0: 12500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sc = Counter(train['sentiment'])\n",
    "pc = Counter(train['polarity'])\n",
    "tsc = Counter(test['sentiment'])\n",
    "tpc = Counter(test['polarity'])\n",
    "print('train sentiment:',sc)\n",
    "print('train polarity:',pc)\n",
    "print('test polarity:',sc)\n",
    "print('test polarity:',pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir    = os.path.join(bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "We increased the length of vectors for 512. \n",
    "\n",
    "Following functions are a simplification of MovieReviewData class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = \"sentence\"\n",
    "LABEL_COLUMN = \"polarity\"\n",
    "global_max = 512                # Global allowed length by bert layer\n",
    "\n",
    "def tokener(text,tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return token_ids\n",
    "\n",
    "def padder(input_ids,maxlen):\n",
    "    input_ids = input_ids[:min(len(input_ids), maxlen - 2)]\n",
    "    input_ids = input_ids + [0] * (maxlen - len(input_ids))\n",
    "    return np.array(input_ids)\n",
    "\n",
    "def prepare(df):\n",
    "    global global_max\n",
    "    tqdm.pandas()\n",
    "    tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "    tokens = df[DATA_COLUMN].progress_apply(lambda text: tokener(text,tokenizer))\n",
    "    maxlen = min(max(len(x) for x in tokens.values),global_max)\n",
    "    print('')\n",
    "    print('padded length:',maxlen)\n",
    "    print('')\n",
    "    padded_tokens = tokens.apply(lambda ids: padder(ids,maxlen))\n",
    "    labels = df[LABEL_COLUMN].apply(int)\n",
    "    return np.array(padded_tokens.values.tolist()),np.array(labels.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:43<00:00, 240.77it/s]\n",
      "\n",
      "padded length: 512\n",
      "\n",
      "100%|██████████| 25000/25000 [01:39<00:00, 252.05it/s]\n",
      "\n",
      "padded length: 512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "((train_x, train_y),\n",
    "    (test_x, test_y)) = map(prepare, [train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (25000, 512)\n",
      "   train_y (25000,)\n",
      "    test_x (25000, 512)\n",
      "    test_y (25000,)\n",
      "global max 512\n"
     ]
    }
   ],
   "source": [
    "print(\"   train_x\", train_x.shape)\n",
    "print(\"   train_y\", train_y.shape)\n",
    "print(\"    test_x\", test_x.shape)\n",
    "print('    test_y',test_y.shape)\n",
    "print(\"global max\", global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 512, 768)\n",
      "Done loading 196 BERT weights from: uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f3266f82128> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 512, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(maxlen, adapter_size=64):\n",
    "        with strategy.scope():\n",
    "                with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "                        bc = StockBertConfig.from_json_string(reader.read())\n",
    "                        bert_params = map_stock_config_to_params(bc)\n",
    "                        bert_params.adapter_size = adapter_size\n",
    "                        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "                input_ids      = keras.layers.Input(shape=(maxlen,), dtype='int32', name=\"input_ids\")\n",
    "                output         = bert(input_ids)\n",
    "\n",
    "                print(\"bert shape\", output.shape)\n",
    "                cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "                cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "                logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "                logits = keras.layers.Dropout(0.5)(logits)\n",
    "                logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
    "\n",
    "                model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "                model.build(input_shape=(None, maxlen))\n",
    "\n",
    "                load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "                # freeze weights if adapter-BERT is used\n",
    "                if adapter_size is not None:\n",
    "                        freeze_bert_layers(bert)\n",
    "\n",
    "                model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "                model.summary()\n",
    "\n",
    "                return model\n",
    "                \n",
    "adapter_size = None # use None to fine-tune all of BERT\n",
    "model = create_model(512, adapter_size=adapter_size)\n",
    "#model_s2 = create_model(s2_ml, adapter_size=adapter_size)\n",
    "#model_s3 = create_model(s3_ml, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(\n",
    "                math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "Removed tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "  2/469 [..............................] - ETA: 50s - loss: 0.7018 - acc: 0.5417WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0112s vs `on_train_batch_end` time: 0.2019s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0112s vs `on_train_batch_end` time: 0.2019s). Check your callbacks.\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7186 - acc: 0.5061WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0654s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_test_batch_end` time: 0.0654s). Check your callbacks.\n",
      "469/469 [==============================] - 144s 307ms/step - loss: 0.7186 - acc: 0.5061 - val_loss: 0.6754 - val_acc: 0.5916\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 102s 217ms/step - loss: 0.6538 - acc: 0.6111 - val_loss: 0.4774 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 103s 219ms/step - loss: 0.4450 - acc: 0.8663 - val_loss: 0.4135 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 103s 219ms/step - loss: 0.4117 - acc: 0.8982 - val_loss: 0.4039 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 103s 219ms/step - loss: 0.3997 - acc: 0.9098 - val_loss: 0.4064 - val_acc: 0.9044\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 103s 219ms/step - loss: 0.3916 - acc: 0.9190 - val_loss: 0.3913 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 103s 220ms/step - loss: 0.3836 - acc: 0.9273 - val_loss: 0.4075 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 103s 220ms/step - loss: 0.3761 - acc: 0.9353 - val_loss: 0.4010 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 103s 219ms/step - loss: 0.3708 - acc: 0.9412 - val_loss: 0.3910 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 103s 220ms/step - loss: 0.3677 - acc: 0.9437 - val_loss: 0.4013 - val_acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3264ac9ba8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_epoch_count = 10\n",
    "model.fit(x=train_x, y=train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=48,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "                                                    end_learn_rate=1e-7,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                   total_epoch_count=total_epoch_count)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "We get 91.63 % accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/782 [..............................] - ETA: 27:15 - loss: 0.3329 - acc: 0.9844WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_test_batch_end` time: 0.0514s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_test_batch_end` time: 0.0514s). Check your callbacks.\n",
      "782/782 [==============================] - 46s 59ms/step - loss: 0.3662 - acc: 0.9455\n",
      "  2/782 [..............................] - ETA: 44s - loss: 0.4039 - acc: 0.9062WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0051s vs `on_test_batch_end` time: 0.0505s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0051s vs `on_test_batch_end` time: 0.0505s). Check your callbacks.\n",
      "782/782 [==============================] - 42s 53ms/step - loss: 0.3947 - acc: 0.9163\n",
      "First Model Acc:\n",
      "train acc 0.9455199837684631\n",
      " test acc 0.9162799715995789\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = model.evaluate(train_x, train_y)\n",
    "_, test_acc = model.evaluate(test_x, test_y)\n",
    "\n",
    "print('First Model Acc:')\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
